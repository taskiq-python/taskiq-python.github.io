const g=(o,a)=>{const i=o.toLowerCase(),e=a.toLowerCase(),s=[];let n=0,l=0;const c=(t,p=!1)=>{let r="";l===0?r=t.length>20?`… ${t.slice(-20)}`:t:p?r=t.length+l>100?`${t.slice(0,100-l)}… `:t:r=t.length>20?`${t.slice(0,20)} … ${t.slice(-20)}`:t,r&&s.push(r),l+=r.length,p||(s.push(["strong",a]),l+=a.length,l>=100&&s.push(" …"))};let h=i.indexOf(e,n);if(h===-1)return null;for(;h>=0;){const t=h+e.length;if(c(o.slice(n,h)),n=t,l>100)break;h=i.indexOf(e,n)}return l<100&&c(o.slice(n),!0),s},d=Object.entries,y=Object.keys,f=o=>o.reduce((a,{type:i})=>a+(i==="title"?50:i==="heading"?20:i==="custom"?10:1),0),$=(o,a)=>{var i;const e={};for(const[s,n]of d(a)){const l=((i=a[s.replace(/\/[^\\]*$/,"")])==null?void 0:i.title)||"",c=`${l?`${l} > `:""}${n.title}`,h=g(n.title,o);h&&(e[c]=[...e[c]||[],{type:"title",path:s,display:h}]),n.customFields&&d(n.customFields).forEach(([t,p])=>{p.forEach(r=>{const u=g(r,o);u&&(e[c]=[...e[c]||[],{type:"custom",path:s,index:t,display:u}])})});for(const t of n.contents){const p=g(t.header,o);p&&(e[c]=[...e[c]||[],{type:"heading",path:s+(t.slug?`#${t.slug}`:""),display:p}]);for(const r of t.contents){const u=g(r,o);u&&(e[c]=[...e[c]||[],{type:"content",header:t.header,path:s+(t.slug?`#${t.slug}`:""),display:u}])}}}return y(e).sort((s,n)=>f(e[s])-f(e[n])).map(s=>({title:s,contents:e[s]}))},m=JSON.parse("{\"/\":{\"/\":{\"title\":\"Task manager for asyncio\",\"contents\":[{\"header\":\"What is taskiq in a nutshell\",\"slug\":\"what-is-taskiq-in-a-nutshell\",\"contents\":[\"Consider taskiq as an asyncio celery implementation. It uses almost the same patterns, but it's more modern and flexible.\",\"It's not a drop-in replacement for any other task manager. It has a different ecosystem of libraries and a different set of features. Also, it doesn't work for synchronous projects. You won't be able to send tasks synchronously.\"]},{\"header\":\"Installation\",\"slug\":\"installation\",\"contents\":[\"You can install taskiq with pip or your favorite dependency manager:\",\"pip install taskiq\\n\"]}]},\"/available-components/\":{\"title\":\"Available components\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"In this section, you can find a list of officially supported plugins for the taskiq.\",\"Available brokers\",\"Available middlewares\",\"Available result backends\",\"Available schedule sources\"]}]},\"/available-components/brokers.html\":{\"title\":\"Available brokers\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"In this section we'll list officially supported brokers.\"]},{\"header\":\"InMemoryBroker\",\"slug\":\"inmemorybroker\",\"contents\":[\"This is a special broker for local development. It uses the same functions to execute tasks, but all tasks are executed locally in the current thread. By default it uses InMemoryResultBackend but this can be overridden.\"]},{\"header\":\"ZeroMQBroker\",\"slug\":\"zeromqbroker\",\"contents\":[\"This broker uses ZMQ to communicate between worker and client processes. It's suitable for small projects with only ONE worker process, because of the ZMQ architecture.\",\"It publishes messages on the local port. All worker processes are reading messages from this port. If you run many worker processes, all tasks will be executed N times, where N is the total number of worker processes.\",\"Be careful!\",\"If you choose this type of broker, please run taskiq with -w 1 parameter, otherwise you may encounter undefined behavior.\",\"To run this broker please install the pyzmq lib. Or you can taskiq with zmq extra.\"]},{\"header\":\"Async shared broker and shared tasks\",\"slug\":\"async-shared-broker-and-shared-tasks\",\"contents\":[\"This is also a special broker. You cannot use it directly. It's used to create shared tasks. These tasks can be imported along with user defined tasks. To define a shared task please use this broker.\",\"from taskiq.brokers.shared_broker import async_shared_broker @async_shared_broker.task def my_task() -> bool: return True \",\"To kiq this task you have to options:\",\"Explicitly define broker using kicker for this kiq;\",\"Add default broker for all shared tasks.\"]},{\"header\":\"Custom brokers\",\"slug\":\"custom-brokers\",\"contents\":[\"These brokers are not parts of the core taskiq lib. You can install them as a separate packages.\",\"You can read more about parameters and abilities of these brokers in README.md of each repo.\"]},{\"header\":\"AioPikaBroker (for RabbitMQ)\",\"slug\":\"aiopikabroker-for-rabbitmq\",\"contents\":[\"Project link: taskiq-aio-pika.\",\"pip install taskiq-aio-pika \"]},{\"header\":\"Redis broker\",\"slug\":\"redis-broker\",\"contents\":[\"Project link: taskiq-redis.\",\"pip install taskiq-redis \"]},{\"header\":\"NATS broker\",\"slug\":\"nats-broker\",\"contents\":[\"Project link: taskiq-nats.\",\"pip install taskiq-nats \"]}]},\"/available-components/middlewares.html\":{\"title\":\"Available middlewares\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"Middlewares allow you to execute code when specific event occurs. Taskiq has several default middlewares.\"]},{\"header\":\"Simple retry middleware\",\"slug\":\"simple-retry-middleware\",\"contents\":[\"This middleware allows you to restart functions on errors. If exception was raised during task execution, the task would be resent with same parameters.\",\"To enable this middleware, add it to the list of middlewares for a broker.\",\"from taskiq import ZeroMQBroker, SimpleRetryMiddleware broker = ZeroMQBroker().with_middlewares( SimpleRetryMiddleware(default_retry_count=3), ) \",\"After that you can add a label to task that you want to restart on error.\",\" @broker.task(retry_on_error=True, max_retries=20) async def test(): raise Exception(\\\"AAAAA!\\\") \",\"retry_on_error enables retries for a task. max_retries is the maximum number of times,.\"]},{\"header\":\"Prometheus middleware\",\"slug\":\"prometheus-middleware\",\"contents\":[\"You can enable prometheus metrics for workers by adding PrometheusMiddleware. To do so, you need to install prometheus_client package or you can install metrics extras for taskiq.\",\"from taskiq import ZeroMQBroker, PrometheusMiddleware broker = ZeroMQBroker().with_middlewares( PrometheusMiddleware(server_addr=\\\"0.0.0.0\\\", server_port=9000), ) \",\"After that, metrics will be available at port 9000. Of course, this parameter can be configured.\\nIf you have other metrics, they'll be shown as well.\"]}]},\"/available-components/result-backends.html\":{\"title\":\"Available result backends\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"Result backends are used to store execution results. This includes:\",\"return value;\",\"Execution time in seconds.\"]},{\"header\":\"DummyResultBackend\",\"slug\":\"dummyresultbackend\",\"contents\":[\"This result backend doesn't do anything. It doesn't store results and cannot be used in cases, where you need actual results.\",\"This broker will always return None for any return_value. Please be careful.\"]},{\"header\":\"Redis result backend\",\"slug\":\"redis-result-backend\",\"contents\":[\"This result backend is not part of the core taskiq library. You can install it as a separate package taskiq-redis.\",\"pip install taskiq-redis \",\"You can read more about parameters and abilities of this broker in README.md.\"]}]},\"/available-components/schedule-sources.html\":{\"title\":\"Available schedule sources\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"These objects are used to fetch current schedule for tasks. Currently we have only one schedule source.\"]},{\"header\":\"LabelScheduleSource\",\"slug\":\"labelschedulesource\",\"contents\":[\"This source parses labels of tasks, and if it finds a schedule label, it considers this task as scheduled.\",\"The format of the schedule label is the following:\",\"@broker.task( schedule=[ { \\\"cron\\\": \\\"* * * * *\\\", # type: str, required argument. \\\"args\\\": [], # type List[Any] | None, can be omitted. \\\"kwargs\\\": {}, # type: Dict[str, Any] | None, can be omitted. \\\"labels\\\": {}, # type: Dict[str, Any] | None, can be omitted. } ] ) async def my_task(): ... \",\"Parameters:\",\"cron - crontab string when to run the task.\",\"args - args to use, when invoking the task.\",\"kwargs - key-word arguments to use when invoking the task.\",\"labels - additional labels to use when invoking the task.\",\"Usage:\",\"from taskiq.scheduler import TaskiqScheduler from taskiq.schedule_sources import LabelScheduleSource broker = ... scheduler = TaskiqScheduler( broker=broker, sources=[LabelScheduleSource(broker)], ) \",\"Cool notice!\",\"In order to resolve all labels correctly, don't forget to import\\nall task modules using CLI interface.\"]}]},\"/extending-taskiq/\":{\"title\":\"Extending taskiq\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"Taskiq is super extendable. The core library comes with different abstract classes. You can implement these abstract classes to extend functionality.\",\"All abstract classes can be found in taskiq.abc package.\"]},{\"header\":\"Contents:\",\"slug\":\"contents\",\"contents\":[\"Brokers\",\"Middlewares\",\"Result backends\",\"CLI\",\"Schedule sources\"]}]},\"/extending-taskiq/broker.html\":{\"title\":\"Brokers\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"To add a new broker you need to implement two methods kick and listen of the taskiq.abc.broker.AsyncBroker abstract class. But along with them we have helper methods. Such as shutdown and startup.\",\"Here is a template for new brokers:\",\"from typing import AsyncGenerator, Union from taskiq import AckableMessage, AsyncBroker, BrokerMessage class MyBroker(AsyncBroker): def __init__(self) -> None: # Please call this super method to set default values to # many different fields. super().__init__() async def startup(self) -> None: # Here you can do some startup magic. # Like opening a connection. return await super().startup() async def shutdown(self) -> None: # Here you can perform shutdown operations. # Like closing connections. return await super().shutdown() async def kick(self, message: BrokerMessage) -> None: # Send a message.message. pass async def listen(self) -> AsyncGenerator[Union[bytes, AckableMessage], None]: while True: # Get new message. new_message: bytes = ... # type: ignore # Yield it! yield new_message \",\"The kick method takes a BrokerMessage as a parameter. The BrokerMessage class is a handy helper class for brokers. You can use information from the BrokerMessage to alter the delivery method.\",\"\\\"cool warning!\\\"\",\"As a broker developer, please send only raw bytes from the message field of a BrokerMessage if possible. Serializing it to the string may result in a problem if message bytes are not utf-8 compatible.\"]},{\"header\":\"Acknowledgement\",\"slug\":\"acknowledgement\",\"contents\":[\"The listen method should yield raw bytes of a message. But if your broker supports acking or rejecting messages, the broker should return taskiq.AckableMessage with required fields.\",\"For example:\",\" async def listen(self) -> AsyncGenerator[AckableMessage, None]: for message in self.my_channel: yield AckableMessage( data=message.bytes, # Ack is a function that takes no parameters. # So you either set here method of a message, # or you can make a closure. ack=message.ack # Can be set to None if broker doesn't support it. reject=message.reject ) \"]},{\"header\":\"Conventions\",\"slug\":\"conventions\",\"contents\":[\"For brokers, we have several conventions. It's good if your broker implements them. These rules are optional, and it's ok if your broker doesn't implement them.\",\"If the message has the delay label with int or float number, this task's execution must be delayed with the same number of seconds as in the delay label.\",\"If the message has the priority label, this message must be sent with priority. Tasks with\\nhigher priorities are executed sooner.\"]}]},\"/extending-taskiq/cli.html\":{\"title\":\"CLI\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"You can easily add new subcommands to taskiq. All default subcommands also use this mechanism, since it's easy to use.\",\"At first you need to add a class that implements taskiq.abc.cmd.TaskiqCMD abstract class.\",\"from argparse import ArgumentParser from typing import Sequence from taskiq.abc.cmd import TaskiqCMD class MyCommand(TaskiqCMD): short_help = \\\"Demo command\\\" def exec(self, args: Sequence[str]) -> None: parser = ArgumentParser() parser.add_argument( \\\"--test\\\", dest=\\\"test\\\", default=\\\"default\\\", help=\\\"My test parameter.\\\", ) parsed = parser.parse_args(args) print(parsed) \",\"In the exec method, you should parse incoming arguments. But since all CLI arguments to taskiq are shifted you can ignore the args parameter.\",\"Also, you can use your favorite tool to build CLI, like click or typer.\",\"After you have such class, you need to add entrypoint that points to that class.\",\"You can read more about entry points in python documentation. The subcommand name is the same as the name of the entry point you've created.\",\"$ taskiq demo --help usage: demo [-h] [--test TEST] optional arguments: -h, --help show this help message and exit --test TEST My test parameter. \",\"$ taskiq demo --test aaa Namespace(test='aaa') \"]}]},\"/extending-taskiq/middleware.html\":{\"title\":\"Middlewares\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"Middlewares are super helpful. You can inject some code before or after task's execution.\",\"Middlewares must implement taskiq.abc.middleware.TaskiqMiddleware abstract class. Every method of a middleware can be either sync or async. Taskiq will execute it as you expect.\",\"For example:\",\"Also, middlewares always have reference to the current broker in self.broker field. If you want to kick a message during the execution of some middleware hooks, you may use self.broker to do so.\",\"Taskiq-pipelines uses middlewares to\\ncall next tasks.\"]}]},\"/extending-taskiq/resutl-backend.html\":{\"title\":\"Result backend\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"Result backends are used to store information about task execution. To create new result_backend you have to implement taskiq.abc.result_backend.AsyncResultBackend class.\",\"Here's a minimal example of a result backend:\",\"from typing import TypeVar from taskiq import TaskiqResult from taskiq.abc.result_backend import AsyncResultBackend _ReturnType = TypeVar(\\\"_ReturnType\\\") class MyResultBackend(AsyncResultBackend[_ReturnType]): async def startup(self) -> None: \\\"\\\"\\\"Do something when starting broker.\\\"\\\"\\\" async def shutdown(self) -> None: \\\"\\\"\\\"Do something on shutdown.\\\"\\\"\\\" async def set_result( self, task_id: str, result: TaskiqResult[_ReturnType], ) -> None: \\\"\\\"\\\" Set result in your backend. :param task_id: current task id. :param result: result of execution. \\\"\\\"\\\" async def get_result( self, task_id: str, with_logs: bool = False, ) -> TaskiqResult[_ReturnType]: \\\"\\\"\\\" Here you must retrieve result by id. Logs is a part of a result. Here we have a parameter whether you want to fetch result with logs or not, because logs can have a lot of info and sometimes it's critical to get only needed information. :param task_id: id of a task. :param with_logs: whether to fetch logs. :return: result. \\\"\\\"\\\" return ... # type: ignore async def is_result_ready( self, task_id: str, ) -> bool: \\\"\\\"\\\" Check if result exists. This function must check whether result is available in your result backend without fetching the result. :param task_id: id of a task. :return: True if result is ready. \\\"\\\"\\\" return ... # type: ignore \",\"Cool tip!\",\"It's a good practice to skip fetching logs from the storage unless with_logs=True is explicitly specified.\",\"Important note!\",\"with_logs param is now deprecated. It will be removed in future releases.\"]}]},\"/extending-taskiq/schedule-sources.html\":{\"title\":\"Schedule source\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"Schedule sources are used to get schedule for tasks. To create new schedule source you have to implement the taskiq.abc.schedule_source.ScheduleSource abstract class.\",\"Here's a minimal example of a schedule source:\",\"from typing import List from taskiq import ScheduledTask, ScheduleSource class MyScheduleSource(ScheduleSource): async def startup(self) -> None: \\\"\\\"\\\"Do something when starting broker.\\\"\\\"\\\" async def shutdown(self) -> None: \\\"\\\"\\\"Do something on shutdown.\\\"\\\"\\\" async def get_schedules(self) -> List[\\\"ScheduledTask\\\"]: # Here you must return list of scheduled tasks from your source. return [ ScheduledTask( task_name=\\\"\\\", labels={}, args=[], kwargs={}, cron=\\\"* * * * *\\\", ), ] # This method is optional. You may not implement this. # It's just a helper to people to be able to interact with your source. async def add_schedule(self, schedule: \\\"ScheduledTask\\\") -> None: return await super().add_schedule(schedule) \"]}]},\"/framework_integrations/\":{\"title\":\"Framework integrations\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"Taskiq is meant to be simple and adaptive. That's why we try to add different integrations to make development with taskiq and your favorite framework easy and fun!\",\"Integrations with frameworks add two things:\",\"Startup and Shutdown events;\",\"Dependencies to use in your handler.\"]}]},\"/framework_integrations/taskiq-with-aiohttp.html\":{\"title\":\"Taskiq + AioHTTP\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"AioHTTP is a framework for building robust applications. We created several libraries to make the experience with AioHTTP even better.\",\"We created a library aiohttp-deps to add FastAPI-like dependency injection in AioHTTP.\",\"To install it, simply run:\",\"pip install \\\"aiohttp-deps\\\" \",\"After the installation, please add startup event to your application to initialize dependencies context.\",\"from aiohttp import web import aiohttp_deps app = web.Application() # This startup event makes all the magic happen. # It parses current handlers and create dependency graphs for them. app.on_startup.append(aiohttp_deps.init) web.run_app(app) \",\"You can read more about dependency injection and available dependencies in the project's README.md.\"]},{\"header\":\"Adding taskiq integration\",\"slug\":\"adding-taskiq-integration\",\"contents\":[\"We highly recommend using aiohttp with aiohttp-deps because it allows us to reuse the same dependencies for your handlers and tasks. First of all, you should install the taskiq-aiohttp library.\",\"pip install \\\"taskiq-aiohttp\\\" \",\"After the installation is complete, add an initialization function call to your broker's main file so it becomes something like this:\",\"import taskiq_aiohttp broker = MyBroker() # The second argument is a path to web.Application variable. # Also you can provide here a factory function that takes no # arguments and returns an application. This function can be async. taskiq_aiohttp.init(broker, \\\"my_project.main:app\\\") \",\"From this point, you'll be able to reuse the same dependencies as with aiohttp-deps. Let's take a look at this function:\",\"from aiohttp import web from taskiq import TaskiqDepends from my_project.tkq import broker @broker.task async def my_task(app: web.Application = TaskiqDepends()): ... \",\"In this example, we depend on the current application. We can use its state in a current task or any other dependency. We can take db_pool from your application's state, which is the same pool, as the one you've created on AiohTTP's startup. But this application is only a mock of your application. It has correct types and all your variables that you filled on startup, but it doesn't handle any request. This integration adds two main dependencies:\",\"web.Application - current application.\",\"web.Request - mocked request. This request only exists to be able to use the same dependencies.\",\"You can find more detailed examples in the examples repo.\"]},{\"header\":\"Testing\",\"slug\":\"testing\",\"contents\":[\"Writing tests for AioHTTP with taskiq is as easy as writing tests for the aiohttp application. The only difference is that, if you want to use InMemoryBroker, then you need to add context for dependency injection. It's easier to call populate_context when creating a test_client fixture.\",\"import taskiq_aiohttp @pytest.fixture async def test_client( app: web.Application, ) -> AsyncGenerator[TestClient, None]: \\\"\\\"\\\" Create a test client. This function creates a TestServer and a test client for the application. Also this fixture populates context with needed variables. :param app: current application. :yield: ready to use client. \\\"\\\"\\\" loop = asyncio.get_running_loop() server = TestServer(app) client = TestClient(server, loop=loop) await client.start_server() # This is important part. # Since InMemoryBroker doesn't # run as a worker process, we have to populate # broker's context by hand. taskiq_aiohttp.populate_context( broker=broker, server=server.runner.server, app=app, loop=loop, ) yield client broker.custom_dependency_context = {} await client.close() \"]}]},\"/framework_integrations/taskiq-with-fastapi.html\":{\"title\":\"Taskiq + FastAPI\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"FastAPI is a highly popular async web framework in Python. It has gained its popularity because of two things:\",\"It's easy to use;\",\"Cool dependency injection.\",\"In taskiq, we try to make our libraries easy to use, and We have a dependency injection too. So we have created the library \\\"taskiq-fastapi\\\" to make integration with FastAPI as smooth as possible.\",\"Let's see what we got here. In this library, we provide users with only one public function called init. It takes a broker and a string path (as in uvicorn) to the fastapi application (or factory function). People should call this function in their main broker file.\",\"from taskiq import ZeroMQBroker import taskiq_fastapi broker = ZeroMQBroker() taskiq_fastapi.init(broker, \\\"my_package.application:app\\\") \",\"There are two rules to make everything work as you expect:\",\"Add TaskiqDepends as a default value for every parameter with Request or HTTPConnection types in base dependencies.\",\"Use only TaskiqDepends in tasks.\",\"Cool and important note!\",\"The Request or HTTPConnection that you'll get injected in your task is not the same request or connection you have had in your handler when you were sending the task!\",\"Many fastapi dependency functions depend on fastapi.Request. We provide a mocked request to such dependencies. But taskiq cannot resolve dependencies until you explicitly specify that this parameter must be injected.\",\"As an example. If you previously had a dependency like this:\",\"from fastapi import Request from typing import Any def get_redis_pool(request: Request) -> Any: return request.app.state.redis_pool \",\"To make it resolvable in taskiq, people should add TaskiqDepends as a default value for each parameter. Like this:\",\"from fastapi import Request from taskiq import TaskiqDepends async def get_redis_pool(request: Request = TaskiqDepends()): return request.app.state.redis_pool \",\"Also you want to call startup of your brokers somewhere.\",\"from fastapi import FastAPI from your_project.taskiq import broker app = FastAPI() @app.on_event(\\\"startup\\\") async def app_startup(): if not broker.is_worker_process: await broker.startup() @app.on_event(\\\"shutdown\\\") async def app_shutdown(): if not broker.is_worker_process: await broker.shutdown() \",\"And that's it. Now you can use your taskiq tasks with functions and classes that depend on FastAPI dependenices. You can find bigger examples in the examples repo.\"]},{\"header\":\"Testing\",\"slug\":\"testing\",\"contents\":[\"Testing is no different from general testing advice from articles about testing. But if you use InMemoryBroker in your tests, you need to provide it with a custom dependency context because it doesn't run as a worker process.\",\"Let's imagine that you have a fixture of your application. It returns a new fastapi application to use in tests.\",\" @pytest.fixture def fastapi_app() -> FastAPI: return get_app() \",\"Right after this fixture, we define another one.\",\"import taskiq_fastapi @pytest.fixture(autouse=True) def init_taskiq_deps(fastapi_app: FastAPI): # This is important part. Here we add dependency context, # this thing helps in resolving dependencies for tasks # for inmemory broker. taskiq_fastapi.populate_dependency_context(broker, fastapi_app) yield broker.custom_dependency_context = {} \",\"This fixture has autouse flag, which means it would run on every test automatically.\"]}]},\"/guide/\":{\"title\":\"Introduction\",\"contents\":[{\"header\":\"What is taskiq\",\"slug\":\"what-is-taskiq\",\"contents\":[\"Taskiq is a library that helps you send and process python functions in a distributed manner. For example, you have many heavy to calculate functions you want to execute on another server. You can implement interservice communication by yourself, or you can use Taskiq to make the job done easily.\",\"The core library doesn't have much functionality. It provides two built-in brokers, CLI, basic functionality for creating distributed tasks, and abstractions to extend the taskiq. The main idea of taskiq is to make it modular and easy to extend. We have libraries for many possible use cases, but if you lack something, you can adopt taskiq to fit your needs.\"]},{\"header\":\"Why not use existing libraries?\",\"slug\":\"why-not-use-existing-libraries\",\"contents\":[\"We created this project because we couldn't find any project that can execute and send async functions using distributed queues like RabbitMQ.\",\"You might have seen projects built on top of asyncio that solve a similar problem, but here's a comparison table of the taskiq and other projects.\",\"Feature name\",\"Taskiq\",\"Arq\",\"AioTasks\",\"Actively maintained\",\"✅\",\"✅\",\"❌\",\"Multiple broker backends\",\"✅\",\"❌\",\"✅\",\"Multiple result backends\",\"✅\",\"❌\",\"❌\",\"Have a rich documentation\",\"✅\",\"❌\",\"❌\",\"Startup & Shutdown events\",\"✅\",\"✅\",\"❌\",\"Have ability to abort tasks\",\"❌\",\"✅\",\"❌\",\"Custom serializers\",\"✅\",\"✅\",\"❌\",\"Dependency injection\",\"✅\",\"❌\",\"❌\",\"Task pipelines\",\"✅\",\"✅\",\"❌\",\"Task schedules\",\"✅\",\"✅\",\"❌\",\"Global middlewares\",\"✅\",\"❌\",\"❌\",\"If you have a fully synchronous project, consider using celery or dramatiq instead.\"]}]},\"/guide/architecture-overview.html\":{\"title\":\"Architecture overview\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"Taskiq has very simple structure. On the client side all messages are sent by kickers using brokers. On the worker side all messages received by the broker and results are stored in result backends.\",\"On the sequence diagram it looks like this:\",\"Cool tip!\",\"If you use dark theme and cannot see words on diagram, try switching to light theme and back to dark.\",\"Let's discuss every component.\"]},{\"header\":\"Broker\",\"slug\":\"broker\",\"contents\":[\"Brokers are the most critical element of the taskiq. Every broker must implement the AsyncBroker abstract class from taskiq.abc.broker to make things work.\",\"AsyncBroker class has two main methods to implement:\",\"kick\",\"listen\",\"The kick method puts the message in the external system. For example, it may call the PUB command in Redis.\",\"The listen is a method with an infinite loop that reads messages from the external system and creates a task for processing messages. For example, it subscribes to the Redis channel and waits for new messages.\"]},{\"header\":\"Kicker\",\"slug\":\"kicker\",\"contents\":[\"Kicker is an object that used to form a message for broker. This class isn't extendable. To form a message kicker uses labels, task name and arguments.\",\"When you call the task.kiq on a task, it generates a Kicker instance and is a shortening for the task.kicker().kiq(...). You can use kicker to change broker, add labels, or even change task_id.\",\"import asyncio from taskiq.brokers.inmemory_broker import InMemoryBroker broker = InMemoryBroker() second_broker = InMemoryBroker() @broker.task async def my_async_task() -> None: \\\"\\\"\\\"My lovely task.\\\"\\\"\\\" await asyncio.sleep(1) print(\\\"Hello\\\") async def main(): # This task was initially assigned to broker, # but this time it is going to be sent using # the second broker with additional label `delay=1`. task = await my_async_task.kicker().with_broker(second_broker).with_labels(delay=1).kiq() print(await task.get_result()) asyncio.run(main()) \"]},{\"header\":\"Result backend\",\"slug\":\"result-backend\",\"contents\":[\"This part is used to store and get results of the execution. Results have type TaskiqResult from taskiq.result.\",\"Every ResultBackend must implement AsyncResultBackend from taskiq.abc.result_backend. By default, brokers use DummyResultBackend. It doesn't do anything and cannot be used in real-world scenarios. But some brokers can override it. For example InMemoryBroker by default uses InMemoryResultBackend and returns correct results.\"]},{\"header\":\"Workers\",\"slug\":\"workers\",\"contents\":[\"Taskiq has a command line interface to run workers. It's simple to get it to work.\",\"You have to provide a path to your broker. As an example, if you want to start listening to new tasks with a broker that is stored in a variable my broker in the module my_project.broker run this in your terminal:\",\"taskiq worker my_project.broker:mybroker \",\"taskiq can discover task modules to import automatically, if you add the -fsd (file system discover) option.\",\"Let's assume we have project with the following structure:\",\"test_project ├── broker.py ├── submodule │ └── tasks.py └── utils └── tasks.py \",\"You can specify all tasks modules to import manually.\",\"taskiq worker test_project.broker:broker test_project.submodule.tasks test_project.utils.tasks \",\"Or you can let taskiq find all python modules named tasks in current directory recursively.\",\"taskiq worker test_project.broker:broker -fsd \",\"If you have uvloop installed, taskiq will automatically install new policies to event loop. You can get more info about the CLI in the CLI section.\"]},{\"header\":\"Middlewares\",\"slug\":\"middlewares\",\"contents\":[\"Middlewares are used to modify message, or take some actions before or after task is complete.\",\"You can write your own middlewares by subclassing the taskiq.abc.middleware.TaskiqMiddleware.\",\"Every hook can be sync or async. Taskiq will execute it.\",\"For example, this is a valid middleware.\",\"import asyncio from taskiq.abc.middleware import TaskiqMiddleware from taskiq.message import TaskiqMessage class MyMiddleware(TaskiqMiddleware): async def pre_send(self, message: \\\"TaskiqMessage\\\") -> TaskiqMessage: await asyncio.sleep(1) message.labels[\\\"my_label\\\"] = \\\"my_value\\\" return message def post_send(self, message: \\\"TaskiqMessage\\\") -> None: print(f\\\"Message {message} was sent.\\\") \",\"Here are methods you can implement in the order they are executed:\",\"pre_send - executed on the client side before the message is sent. Here you can modify the message.\",\"post_send - executed right after the message was sent.\",\"pre_execute - executed on the worker side after the message was received by a worker and before its execution.\",\"on_error - executed after the task was executed if the exception was found.\",\"post_execute - executed after the message was executed.\",\"post_save - executed after the result was saved in the result backend.\",\"You can use sync or async hooks without changing anything, but adding async to the hook signature.\",\"important note\",\"If exception happens in middlewares it won't be caught. Please ensure that you have try\\\\except for all edge cases of your middleware.\",\"Middlewares can store information in message.labels for later use. For example SimpleRetryMiddleware uses labels to remember number of failed attempts.\"]},{\"header\":\"Messages\",\"slug\":\"messages\",\"contents\":[\"Every message has labels. You can define labels using task decorator, or you can add them using kicker.\",\"For example:\",\" @broker.task(my_label=1, label2=\\\"something\\\") async def my_async_task() -> None: \\\"\\\"\\\"My lovely task.\\\"\\\"\\\" await asyncio.sleep(1) print(\\\"Hello\\\") async def main(): await my_async_task.kiq() \",\"It's equivalent to this\",\" @broker.task async def my_async_task() -> None: \\\"\\\"\\\"My lovely task.\\\"\\\"\\\" await asyncio.sleep(1) print(\\\"Hello\\\") async def main(): await my_async_task.kicker().with_labels( my_label=1, label2=\\\"something\\\", ).kiq() \",\"Also you can assign custom task names using decorator. This is useful to be sure that task names are unique and resolved correctly. Also it may be useful to balance message routing in some brokers.\",\"for example:\",\"@broker.task(task_name=\\\"my_tasks.add_one\\\", label1=1) async def my_async_task() -> None: \\\"\\\"\\\"My lovely task.\\\"\\\"\\\" await asyncio.sleep(1) print(\\\"Hello\\\") \"]},{\"header\":\"Context\",\"slug\":\"context\",\"contents\":[\"This section is useful for library developers. Who want to get current broker during shared task execution.\",\"For example, you've created shared_task and you want to send message in that task. This can be done with context.\",\"Context holds information about the current broker and current incoming message. To get it, simply add the context parameter with type-hint.\",\"Cool warning!\",\"Context injected only if you have a type hint.\",\"Example:\",\"from taskiq import async_shared_broker, BrokerMessage, Context @async_shared_broker.task async def my_shr_task(context: Context): message = BrokerMessage( task_id=\\\"123\\\", task_name=\\\"dummy_name\\\", message='{\\\"one\\\": \\\"two\\\"}', labels={}, ) await context.broker.kick(message) \",\"This is useless example, but it's good as a demonstration.\\nPipelines are built using this magic.\"]}]},\"/guide/cli.html\":{\"title\":\"CLI\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"Core library comes with CLI program called taskiq, which is used to run different subcommands.\",\"By default taskiq is shipped with only two commands: worker and scheduler. You can search for more taskiq plugins using pypi. Some plugins may add new commands to taskiq.\"]},{\"header\":\"Worker\",\"slug\":\"worker\",\"contents\":[\"To run worker process, you have to specify the broker you want to use and modules with defined tasks. Like this:\",\"taskiq worker mybroker:broker_var my_project.module1 my_project.module2 \"]},{\"header\":\"Auto importing\",\"slug\":\"auto-importing\",\"contents\":[\"Enumerating all modules with tasks is not an option sometimes. That's why taskiq can auto-discover tasks in current directory recursively.\",\"We have two options for this:\",\"--tasks-pattern or -tp. It's a name of files to import. By default is searches for all tasks.py files.\",\"--fs-discover or -fsd. This option enables search of task files in current directory recursively, using the given pattern.\"]},{\"header\":\"Type casts\",\"slug\":\"type-casts\",\"contents\":[\"One of features taskiq have is automatic type casts. For example you have a type-hinted task like this:\",\"async def task(val: int) -> int: return val + 1 \",\"If you'll call task.kiq(\\\"2\\\") you'll get 3 as the returned value. Because we parse signatures of tasks and cast incoming parameters to target types. If type-cast fails you won't throw any error. It just leave the value as is. That functionality allows you to use pydantic models, or dataclasses as the input parameters.\",\"To disable this pass the --no-parse option to the taskiq.\"]},{\"header\":\"Hot reload\",\"slug\":\"hot-reload\",\"contents\":[\"This is annoying to restart workers every time you modify tasks. That's why taskiq supports hot-reload. Reload is unavailable by default. To enable this feature install taskiq with reload extra.\",\"To enable this option simply pass the --reload or -r option to worker taskiq CLI.\",\"Also this option supports .gitignore files. If you have such file in your directory, it won't reload worker when you modify ignored files. To disable this functionality pass --do-not-use-gitignore option.\"]},{\"header\":\"Scheduler\",\"slug\":\"scheduler\",\"contents\":[\"Scheduler is used to schedule tasks as described in Scheduling tasks section.\",\"To run it simply run\",\"taskiq scheduler <path to scheduler> [optional module to import]... \",\"For example\",\"taskiq scheduler my_project.broker:scheduler my_project.module1 my_project.module2 \"]},{\"header\":\"Parameters\",\"slug\":\"parameters\",\"contents\":[\"Path to scheduler is the only required argument.\",\"--tasks-pattern or -tp. It's a name of files to import. By default is searches for all tasks.py files.\",\"--fs-discover or -fsd. This option enables search of task files in current directory recursively, using the given pattern.\",\"--log-level is used to set a log level.\"]}]},\"/guide/getting-started.html\":{\"title\":\"Getting started\",\"contents\":[{\"header\":\"Installation\",\"slug\":\"installation\",\"contents\":[\"You can install taskiq from pypi or directly from git using pip:\",\"After installation of the core library, you need to find the broker that fits you. You can do it using PyPI search.\",\"Cool tip!\",\"We highly recommend taskiq-aio-pika or taskiq-nats as the broker and taskiq-redis as the result backend for production use.\"]},{\"header\":\"Running tasks\",\"slug\":\"running-tasks\",\"contents\":[\"Now you need to create a python module with broker declaration. It's just a plain python file with the variable of your broker. For this particular example, I'm going to use the InMemoryBroker.\",\"Important note\",\"The InMemoryBroker doesn't send any data over the network, and you cannot use this broker in a real-world scenario, but it's still useful for local development if you do not want to set up a taskiq worker.\",\"# broker.py from taskiq import InMemoryBroker broker = InMemoryBroker() \",\"And that's it. Now let's add some tasks and the main function. You can add tasks in separate modules. You can find more information about that further. Also, we call the startup method at the beginning of the main function.\",\"# broker.py import asyncio from taskiq import InMemoryBroker broker = InMemoryBroker() @broker.task async def add_one(value: int) -> int: return value + 1 async def main() -> None: # Never forget to call startup in the beginning. await broker.startup() # Send the task to the broker. task = await add_one.kiq(1) # Wait for the result. result = await task.wait_result(timeout=2) print(f\\\"Task execution took: {result.execution_time} seconds.\\\") if not result.is_err: print(f\\\"Returned value: {result.return_value}\\\") else: print(\\\"Error found while executing task.\\\") await broker.shutdown() if __name__ == \\\"__main__\\\": asyncio.run(main()) \",\"Cool warning!\",\"Calling the startup method is necessary. If you don't call it, you may get an undefined behaviour.\",\"If you run this code, you will get this in your terminal:\",\"❯ python mybroker.py Task execution took: 7.3909759521484375e-06 seconds. Returned value: 2 \",\"Ok, the code of the task execution is a little bit fancier than an ordinary function call, but it's still relatively simple to understand. To send a task to the broker, you need to call the .kiq method on the function, it returns the TaskiqTask object that can check whether the result is ready or not. Also it has methods to wait for the result to become available.\",\"You can get more information about taskiq types, CLI and internal structure in the \\\"Architecture overview\\\" section.\"]},{\"header\":\"Distributed run\",\"slug\":\"distributed-run\",\"contents\":[\"Now let's change InMemoryBroker to some distributed broker instead. In this example we are going to use broker that works with rabbitMQ.\",\"At first we must install the taskiq-aio-pika lib.\",\"pip install taskiq-aio-pika \",\"After the installation, replace the broker we defined earlier with the broker from the taskiq-aio-pika.\",\"from taskiq_aio_pika import AioPikaBroker broker = AioPikaBroker('amqp://guest:guest@localhost:5672') \",\"Also, AioPika broker requires to call startup before using it. Add this line at the beginning of the main function.\",\"await broker.startup() \",\"That's all you need to do.\",\"Complete code\",\"# broker.py import asyncio from taskiq_aio_pika import AioPikaBroker broker = AioPikaBroker(\\\"amqp://guest:guest@localhost:5672\\\") @broker.task async def add_one(value: int) -> int: return value + 1 async def main() -> None: await broker.startup() # Send the task to the broker. task = await add_one.kiq(1) # Wait for the result. result = await task.wait_result(timeout=2) print(f\\\"Task execution took: {result.execution_time} seconds.\\\") if not result.is_err: print(f\\\"Returned value: {result.return_value}\\\") else: print(\\\"Error found while executing task.\\\") await broker.shutdown() if __name__ == \\\"__main__\\\": asyncio.run(main()) \",\"Let's run the worker process. First of all, we need rabbitMQ up and running. I highly recommend you use docker.\",\"Now we need to start worker process by running taskiq cli command. You can get more info about the CLI in the CLI section.\",\"taskiq worker broker:broker \",\"After the worker is up, we can run our script as an ordinary python file and see how the worker executes tasks.\",\"$ python broker.py Task execution took: 0.0 seconds. Returned value: None \",\"But the printed result value is not correct. That happens because we didn't provide any result backend that can store results of task execution. To store results, we can use the taskiq-redis library.\",\"pip install taskiq-redis \",\"After the installation, add a new result backend to the broker.\",\"from taskiq_redis import RedisAsyncResultBackend broker = AioPikaBroker( \\\"amqp://guest:guest@localhost:5672\\\", ).with_result_backend(RedisAsyncResultBackend(\\\"redis://localhost\\\")) \",\"Now we need to start redis.\",\"Complete code\",\"# broker.py import asyncio from taskiq_aio_pika import AioPikaBroker from taskiq_redis import RedisAsyncResultBackend broker = AioPikaBroker( \\\"amqp://guest:guest@localhost:5672\\\", ).with_result_backend(RedisAsyncResultBackend(\\\"redis://localhost\\\")) @broker.task async def add_one(value: int) -> int: return value + 1 async def main() -> None: await broker.startup() # Send the task to the broker. task = await add_one.kiq(1) # Wait for the result. result = await task.wait_result(timeout=2) print(f\\\"Task execution took: {result.execution_time} seconds.\\\") if not result.is_err: print(f\\\"Returned value: {result.return_value}\\\") else: print(\\\"Error found while executing task.\\\") await broker.shutdown() if __name__ == \\\"__main__\\\": asyncio.run(main()) \",\"Let's run taskiq once again. The command is the same.\",\"taskiq worker broker:broker \",\"Now, if we run this file with python, we can get the correct results with a valid execution time.\",\"$ python broker.py Task execution took: 1.0013580322265625e-05 seconds. Returned value: 2 \",\"Continue reading to get more information about taskiq internals.\"]}]},\"/guide/scheduling-tasks.html\":{\"title\":\"Scheduling tasks\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"Sometimes you may want to execute some tasks according to some schedule. For example, you maybe want to call a function every day at 2 pm.\",\"That's not a problem if you use taskiq. We have primitives that can help you to solve your problems.\",\"Let's imagine we have a module, as shown below, and we want to execute the heavy_task every 5 minutes. What should we do?\",\"from taskiq_aio_pika import AioPikaBroker broker = AioPikaBroker(\\\"amqp://guest:guest@localhost:5672/\\\") @broker.task async def heavy_task(value: int) -> int: return value + 1 \",\"Of course we can implement loop like this:\",\" while True: await heavy_task.kiq(1) await asyncio.sleep(timedelta(minutes=5).total_seconds) \",\"But if you have many schedules it may be a little painful to implement. So let me introduce you the TaskiqScheduler. Let's add scheduler to our module.\",\"from taskiq_aio_pika import AioPikaBroker from taskiq.schedule_sources import LabelScheduleSource from taskiq.scheduler import TaskiqScheduler broker = AioPikaBroker(\\\"amqp://guest:guest@localhost:5672/\\\") scheduler = TaskiqScheduler( broker=broker, sources=[LabelScheduleSource(broker)], ) @broker.task(schedule=[{\\\"cron\\\": \\\"*/5 * * * *\\\", \\\"args\\\": [1]}]) async def heavy_task(value: int) -> int: return value + 1 \",\"That's it.\",\"Now we need to start our scheduler with the taskiq scheduler command. Like this:\",\"taskiq scheduler module:scheduler \",\"Be careful!\",\"Please always run only one instance of the scheduler! If you run more than one scheduler at a time, please be careful since it may execute one task N times, where N is the number of running scheduler instances.\",\"This command will import the scheduler you defined and start sending tasks to your broker.\",\"Cool tip!\",\"The scheduler doesn't execute tasks. It only sends them.\",\"You can check list of available schedule sources in the Available schedule sources section.\"]},{\"header\":\"Multiple sources\",\"slug\":\"multiple-sources\",\"contents\":[\"Sometimes you may want to use multiple sources to assemble a schedule for tasks. The TaskiqScheduler can do so. But it's obvious how to merge schedules from different sources.\",\"That's why you can pass a custom merge function to resolve all possible conflicts or if you want to have more complex logic aside from sources. For example, filter out some task schedules.\",\"Currently we have only two default functions to merge tasks. You can find them in the taskiq.scheduler.merge_functions module.\",\"preserve_all - simply adds new schedules to the old ones.\",\"only_unique - adds schedule only if it was not added by previous sources.\",\"Every time we update schedule it gets task from the source and executes this function to merge them together.\"]}]},\"/guide/state-and-deps.html\":{\"title\":\"State and Dependencies\",\"contents\":[{\"header\":\"State\",\"slug\":\"state\",\"contents\":[\"The TaskiqState is a global variable where you can keep the variables you want to use later. For example, you want to open a database connection pool at a broker's startup.\",\"This can be achieved by adding event handlers.\",\"You can use one of these events:\",\"WORKER_STARTUP\",\"CLIENT_STARTUP\",\"WORKER_SHUTDOWN\",\"CLIENT_SHUTDOWN\",\"Worker events are called when you start listening to the broker messages using taskiq. Client events are called when you call the startup method of your broker from your code.\",\"This is an example of code using event handlers:\",\"import asyncio from typing import Optional from redis.asyncio import ConnectionPool, Redis # type: ignore from taskiq_aio_pika import AioPikaBroker from taskiq_redis import RedisAsyncResultBackend from taskiq import Context, TaskiqDepends, TaskiqEvents, TaskiqState # To run this example, please install: # * taskiq # * taskiq-redis # * taskiq-aio-pika broker = AioPikaBroker( \\\"amqp://localhost\\\", ).with_result_backend(RedisAsyncResultBackend(\\\"redis://localhost\\\")) @broker.on_event(TaskiqEvents.WORKER_STARTUP) async def startup(state: TaskiqState) -> None: # Here we store connection pool on startup for later use. state.redis = ConnectionPool.from_url(\\\"redis://localhost/1\\\") @broker.on_event(TaskiqEvents.WORKER_SHUTDOWN) async def shutdown(state: TaskiqState) -> None: # Here we close our pool on shutdown event. await state.redis.disconnect() @broker.task async def get_val(key: str, context: Context = TaskiqDepends()) -> Optional[str]: # Now we can use our pool. redis = Redis(connection_pool=context.state.redis, decode_responses=True) return await redis.get(key) @broker.task async def set_val(key: str, value: str, context: Context = TaskiqDepends()) -> None: # Now we can use our pool to set value. await Redis(connection_pool=context.state.redis).set(key, value) async def main() -> None: await broker.startup() set_task = await set_val.kiq(\\\"key\\\", \\\"value\\\") set_result = await set_task.wait_result(with_logs=True) if set_result.is_err: print(set_result.log) raise ValueError(\\\"Cannot set value in redis. See logs.\\\") get_task = await get_val.kiq(\\\"key\\\") get_res = await get_task.wait_result() print(f\\\"Got redis value: {get_res.return_value}\\\") await broker.shutdown() if __name__ == \\\"__main__\\\": asyncio.run(main()) \",\"Cool tip!\",\"If you want to add handlers programmatically, you can use the broker.add_event_handler function.\",\"As you can see in this example, this worker will initialize the Redis pool at the startup. You can access the state from the context.\"]},{\"header\":\"Dependencies\",\"slug\":\"dependencies\",\"contents\":[\"Using context directly is nice, but this way you won't get code-completion.\",\"That's why we suggest you try TaskiqDependencies. The implementation is very similar to FastApi's dependencies. You can use classes, functions, and generators as dependencies.\",\"We use the taskiq-dependencies package to provide autocompetion. You can easily integrate it in your own project.\",\"Cool alarm!\",\"FastAPI's Depends is not compatible with TaskiqDepends.\"]},{\"header\":\"How dependencies are useful\",\"slug\":\"how-dependencies-are-useful\",\"contents\":[\"You can use dependencies for better autocompletion and reduce the amount of code you write. Since the state is generic, we cannot guess the types of the state fields. Dependencies can be annotated with type hints and therefore provide better auto-completion.\",\"Let's assume that you've stored a Redis connection pool in the state as in the example above.\",\"@broker.on_event(TaskiqEvents.WORKER_STARTUP) async def startup(state: TaskiqState) -> None: # Here we store connection pool on startup for later use. state.redis = ConnectionPool.from_url(\\\"redis://localhost/1\\\") \",\"You can access this variable by using the current execution context directly, like this:\",\"@broker.task async def my_task(context: Context = TaskiqDepends()) -> None: async with Redis(connection_pool=context.state.redis, decode_responses=True) as redis: await redis.set('key', 'value') \",\"If you hit the TAB button after the context.state. expression, your IDE won't give you any auto-completion. But we can create a dependency function to add auto-completion.\",\" def redis_dep(context: Context = TaskiqDepends()) -> Redis: return Redis(connection_pool=context.state.redis, decode_responses=True) @broker.task async def my_task(redis: Redis = TaskiqDepends(redis_dep)) -> None: await redis.set('key', 'value') \",\"Now, this dependency injection will be autocompleted. But, of course, state fields cannot be autocompleted, even in dependencies. But this way, you won't make any typos while writing tasks.\"]},{\"header\":\"How do dependencies work\",\"slug\":\"how-do-dependencies-work\",\"contents\":[\"We build a graph of dependencies on startup. If the parameter of the function has the default value of TaskiqDepends this parameter will be treated as a dependency.\",\"Dependencies can also depend on something. Also dependencies are optimized to not evaluate things many times.\",\"For example:\",\"import random from taskiq import TaskiqDepends def common_dep() -> int: # For example it returns 8 return random.randint(1, 10) def dep1(cd: int = TaskiqDepends(common_dep)) -> int: # This function will return 9 return cd + 1 def dep2(cd: int = TaskiqDepends(common_dep)) -> int: # This function will return 10 return cd + 2 def my_task( d1: int = TaskiqDepends(dep1), d2: int = TaskiqDepends(dep2), ) -> int: # This function will return 19 return d1 + d2 \",\"In this code, the dependency common_dep is going to be evaluated only once and the dep1 and the dep2 are going to receive the same value. You can control this behavior by using the use_cache=False parameter to you dependency. This parameter will force the dependency to reevaluate all it's subdependencies.\",\"In this example we cannot predict the result. Since the dep2 doesn't use cache for the common_dep function.\",\"import random from taskiq import TaskiqDepends def common_dep() -> int: return random.randint(1, 10) def dep1(cd: int = TaskiqDepends(common_dep)) -> int: return cd + 1 def dep2(cd: int = TaskiqDepends(common_dep, use_cache=False)) -> int: return cd + 2 def my_task( d1: int = TaskiqDepends(dep1), d2: int = TaskiqDepends(dep2), ) -> int: return d1 + d2 \",\"The graph for cached dependencies looks like this:\",\"The dependencies graph for my_task where dep2 doesn't use cached value for common_dep looks like this:\"]},{\"header\":\"Class as a dependency\",\"slug\":\"class-as-a-dependency\",\"contents\":[\"You can use classes as dependencies, and they can also use other dependencies too.\",\"Let's see an example:\",\"from taskiq import TaskiqDepends async def db_connection() -> str: return \\\"let's pretend as this is a connection\\\" class MyDAO: def __init__(self, db_conn: str = TaskiqDepends(db_connection)) -> None: self.db_conn = db_conn def get_users(self) -> str: return self.db_conn.upper() def my_task(dao: MyDAO = TaskiqDepends()) -> None: print(dao.get_users()) \",\"As you can see, the dependency for my_task function is declared with TaskiqDependency(). It's because you can omit the class if it's declared in type-hint for the parameter. This feature doesn't work with dependency functions, it's only for classes.\",\"You can pass dependencies for classes in the constructor.\"]},{\"header\":\"Generator dependencies\",\"slug\":\"generator-dependencies\",\"contents\":[\"Generator dependencies are used to perform startup before task execution and teardown after the task execution.\",\"from typing import Generator from taskiq import TaskiqDepends def dependency() -> Generator[str, None, None]: print(\\\"Startup\\\") yield \\\"value\\\" print(\\\"Shutdown\\\") async def my_task(dep: str = TaskiqDepends(dependency)) -> None: print(dep.upper()) \",\"In this example, we can do something at startup before the execution and at shutdown after the task is completed.\",\"If you want to do something asynchronously, convert this function to an asynchronous generator. Like this:\",\"import asyncio from typing import AsyncGenerator from taskiq import TaskiqDepends async def dependency() -> AsyncGenerator[str, None]: print(\\\"Startup\\\") await asyncio.sleep(0.1) yield \\\"value\\\" await asyncio.sleep(0.1) print(\\\"Shutdown\\\") async def my_task(dep: str = TaskiqDepends(dependency)) -> None: print(dep.upper()) \"]},{\"header\":\"Default dependencies\",\"slug\":\"default-dependencies\",\"contents\":[\"By default taskiq has only two dependencies:\",\"Context from taskiq.context.Context\",\"TaskiqState from taskiq.state.TaskiqState\"]},{\"header\":\"Adding first-level dependencies\",\"slug\":\"adding-first-level-dependencies\",\"contents\":[\"You can expand default list of available dependencies for you application. Taskiq have an ability to add new first-level dependencies using brokers.\",\"The AsyncBroker interface has a function called add_dependency_context and you can add more default dependencies to the taskiq. This may be useful for libraries if you want to add new dependencies to users.\"]},{\"header\":\"Exception handling\",\"slug\":\"exception-handling\",\"contents\":[\"Dependencies can handle exceptions that happen in tasks. This feature is handy if you want your system to be more atomic.\",\"For example, if you open a database transaction in your dependency and want to commit it only if the function is completed successfully.\",\"async def get_transaction(db_driver: DBDriver = TaskiqDepends(get_driver)) -> AsyncGenerator[Transaction, None]: trans = db_driver.begin_transaction(): try: # Here we give transaction to our dependant function. yield trans # If exception was found in dependant function, # we rollback our transaction. except Exception: await trans.rollback() return # Here we commit if everything is fine. await trans.commit() \",\"If you don't want to propagate exceptions in dependencies, you can add --no-propagate-errors option to worker command.\",\"taskiq worker my_file:broker --no-propagate-errors \",\"In this case, no exception will ever going to be propagated to any dependency.\"]}]},\"/guide/testing-taskiq.html\":{\"title\":\"Testing with taskiq\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"Every time we write programs, we want them to be correct. To achieve this, we use tests. Taskiq allows you to write tests easily as if tasks were normal functions.\",\"Let's dive into examples.\"]},{\"header\":\"Preparations\",\"slug\":\"preparations\",\"contents\":[]},{\"header\":\"Environment setup\",\"slug\":\"environment-setup\",\"contents\":[\"For testing you maybe don't want to use actual distributed broker. But still you want to validate your logic. Since python is an interpreted language, you can easily replace you broker with another one if the expression is correct.\",\"We can set an environment variable, that indicates that currently we're running in testing environment.\",\"Or we can even tell pytest to set this environment for us, just before executing tests using pytest-env plugin.\"]},{\"header\":\"Async tests\",\"slug\":\"async-tests\",\"contents\":[\"Since taskiq is fully async, we suggest using anyio to run async functions in pytest. Install the lib and place this fixture somewhere in your root conftest.py file.\",\"@pytest.fixture def anyio_backend(): return 'asyncio' \",\"After the preparations are done, we need to modify the broker's file in your project.\",\"import os from taskiq import AsyncBroker, InMemoryBroker, ZeroMQBroker env = os.environ.get(\\\"ENVIRONMENT\\\") broker: AsyncBroker = ZeroMQBroker() if env and env == \\\"pytest\\\": broker = InMemoryBroker() \",\"As you can see, we added an if statement. If the expression is true, we replace our broker with an imemory broker. The main point here is to not have an actual connection during testing. It's useful because inmemory broker has the same interface as a real broker, but it doesn't send tasks actually.\"]},{\"header\":\"Testing tasks\",\"slug\":\"testing-tasks\",\"contents\":[\"Let's define a task.\",\"from your_project.taskiq import broker @broker.task async def parse_int(val: str) -> int: return int(val) \",\"This simple task may be defined anywhere in your project. If you want to test it, just import it and call as a normal function.\",\"import pytest from your_project.tasks import parse_int @pytest.mark.anyio async def test_task(): assert await parse_int(\\\"11\\\") == 11 \",\"And that's it. Test should pass.\",\"What if you want to test a function that uses task. Let's define such function.\",\"from your_project.taskiq import broker @broker.task async def parse_int(val: str) -> int: return int(val) async def parse_and_add_one(val: str) -> int: task = await parse_int.kiq(val) result = await task.wait_result() return result.return_value + 1 \",\"And since we replaced our broker with InMemoryBroker, we can just call it. It would work as you expect and tests should pass.\",\"@pytest.mark.anyio async def test_add_one(): assert await parse_and_add_one(\\\"11\\\") == 12 \"]},{\"header\":\"Dependency injection\",\"slug\":\"dependency-injection\",\"contents\":[\"If you use dependencies in your tasks, you may think that this can become a problem. But it's not. Here's what we came up with. We added a method called add_dependency_context to the broker. It sets base dependencies for dependency resolution. You can use it for tests.\",\"Let's add a task that depends on Path. I guess this example is not meant to be used in production code bases, but it's suitable for illustration purposes.\",\"from pathlib import Path from taskiq import TaskiqDepends from your_project.taskiq import broker @broker.task async def modify_path(some_path: Path = TaskiqDepends()): return some_path.parent / \\\"taskiq.py\\\" \",\"To test the task itself, it's not different to the example without dependencies, but we jsut need to pass all expected dependencies manually as function's arguments or key-word arguments.\",\"import pytest from your_project.taskiq import broker from pathlib import Path @pytest.mark.anyio async def test_modify_path(): modified = await modify_path(Path.cwd()) assert str(modified).endswith(\\\"taskiq.py\\\") \",\"But what if we want to test task execution? Well, you don't need to provide dependencies manually, you must mutate dependency_context before calling a task. We suggest to do it in fixtures.\",\"import pytest from your_project.taskiq import broker from pathlib import Path # We use autouse, so this fixture # is called automatically before all tests. @pytest.fixture(scope=\\\"function\\\", autouse=True) async def init_taskiq_dependencies(): # Here we use Path, but you can use other # pytest fixtures here. E.G. FastAPI app. broker.add_dependency_context({Path: Path.cwd()}) yield # After the test we clear all custom dependencies. broker.custom_dependency_context = {} \",\"This fixture will update dependency context for our broker before every test. Now tasks with dependencies can be used. Let's try it out.\",\"@pytest.mark.anyio async def test_modify_path(): task = await modify_path.kiq() result = await task.wait_result() assert str(result.return_value).endswith(\\\"taskiq.py\\\") \",\"This should pass. And that's it for now.\"]}]}}}");self.onmessage=({data:o})=>{self.postMessage($(o.query,m[o.routeLocale]))};
//# sourceMappingURL=original.js.map
